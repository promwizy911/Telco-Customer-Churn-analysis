# -*- coding: utf-8 -*-
"""Telecom customer churn_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13IZS1ObSKsUxgjTU1Aoi4rsIaQf6g8u6

# **Telecom customer Retention**

# Context:

In the highly competitive telecommunications industry, customer retention is just as critical as customer acquisition. The cost of acquiring a new customer is often significantly higher than the cost of retaining an existing one. This dataset represents the customer base of a fictional telecom provider that is seeking to reduce its "churn rate",the percentage of customers who stop using their services. By analyzing demographic data, service subscriptions, and billing information, the company aims to uncover the underlying patterns that lead to customer departures.

# Objective

The primary objective of this analysis is to develop a data-driven understanding of customer churn. Specifically, the project aims to:

- Identify Risk Factors: Determine which customer attributes (e.g., contract type, internet service, or tenure) are the strongest predictors of churn.

- Predictive Modeling: Build a model to accurately identify high-risk customers before they leave, allowing for proactive intervention.

- Business Insights: Provide actionable recommendations for the marketing and customer success teams, such as identifying which service bundles or payment methods improve customer loyalty.

# Data Descriptions

- customerID: A unique identifier for each customer.

- gender: Whether the customer is male or female.

- SeniorCitizen: Whether the customer is a senior citizen.

- Partner: Whether the customer has a partner.
- Dependents: Whether the customer has dependents.

- tenure: Number of months the customer has stayed with the company.

- PhoneService: Whether the customer has a phone service.

- MultipleLines: Whether the customer has multiple lines.

- InternetService: Customerâ€™s internet service provider.

- OnlineSecurity: Whether the customer has online security.

- OnlineBackup: Whether the customer has online backup.

- DeviceProtection: Whether the customer has device protection.

- TechSupport: Whether the customer has tech support.

- StreamingTV: Whether the customer has streaming TV.

- StreamingMovies: Whether movie streaming service is included.

- Contract: Contract duration.

- PaperlessBilling: Whether the customer uses paperless billing.

- PaymentMethod: How the customer pays.

- MonthlyCharges: Amount charged to the customer per month.

- TotalCharges: Total amount charged to the customer over their tenure.

- Churn: Indicates whether the customer left the company.

#Importing Neccesary Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# To help with reading and manipulating data
import pandas as pd
import numpy as np

# To help with data visualization
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

# To be used for missing value imputation
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer

# To help with model building
from sklearn.linear_model import LogisticRegression,RidgeClassifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    AdaBoostClassifier,
    GradientBoostingClassifier,
    RandomForestClassifier,
    BaggingClassifier,
)
from xgboost import XGBClassifier

# To get different metric scores, and split data
from sklearn import metrics
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    ConfusionMatrixDisplay,
)
#Libraries for oversampling and undersampling imbalance target data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import make_pipeline
from imblearn.pipeline import Pipeline



#Libraries to help with regularization models
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge

# To be used for data scaling and one hot encoding
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

# To be used for tuning the model
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# To be used for creating pipelines and personalizing them

from sklearn.compose import ColumnTransformer
from imblearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer

# To define maximum number of columns to be displayed in a dataframe
pd.set_option("display.max_columns", None)

# To supress scientific notations for a dataframe
pd.set_option("display.float_format", lambda x: "%.3f" % x)

# To supress warnings
import warnings

warnings.filterwarnings("ignore")


from sklearn.compose import ColumnTransformer

import scipy.stats as stats

"""#Loading data set"""

df_main = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn 2.csv") #Loading dataset
# copying data to another variable to avoid any changes to original data
data = df_main.copy()

"""# *Data Overview*

- Obervations
- Sanity check
"""

data.head(7)

data.shape

"""- There are 21 columns and 7043 in the data set"""

# Checking Unique values of each column

unique_values = {
    column: set(data[column].unique())
    for column in data.columns
    if column != "customerID" and not np.issubdtype(data[column].dtype, np.floating)
}

for feature, values in unique_values.items():
    print(f"Unique values for {feature}: {values}")

data['Churn'].value_counts()

"""- The predictor variable is markedly imbalanced, this will affect which model and metric I will be using later in the notebook."""

data.info()

"""- TotalCharge column is stored as an Object instead of a float though there are no null values in the column, this may indicate some missing values in this column."""

missing_total_charges = data["TotalCharges"].isnull().sum()
print(f"There are {missing_total_charges} missing values in the TotalCharges column")

"""- There are no missing or Nan values its likely there are some blank cells in the totalCharge column that is causing the feature to be stored as an object."""

missing_total_charges = (
    data["TotalCharges"].isnull().sum()
    + (data["TotalCharges"].str.strip() == "").sum()
)

print(f"There are {missing_total_charges} missing or blank values in the TotalCharges column")

"""- Confirmed, blank values are the culprit.
- These customers are likely to be new customers who 0 tenure period and thus have met the threshold(probably spending at least 2 months) to have a total charge.
"""

#Checking for missing values
missing_values = data.isnull().sum()
print(f'There are {missing_values.sum()} missing values')

missing_df = pd.DataFrame({
    "Column": data.columns,
    "Missing_or_Blank_Count": [
        data[col].isnull().sum() + (data[col].str.strip() == "").sum()
        if data[col].dtype == "object"
        else data[col].isnull().sum()
        for col in data.columns
    ]
})

missing_df

#Checking for duplicates
duplicates = data.duplicated().sum()
print(f'There are {duplicates} duplicates')

"""##*Statistical Summary*"""

#Checking statistical summary
data.describe(include='all').T

"""- TotalCharge statistical summary is still poorly represented because of its issues with data type highlighted earlier.

#Exploratory data analysis
"""

#Creating a copy of dataset for EDA
data1 = data.copy()
#dropping unique identifier column 'customerID'
data1 = data1.drop('customerID', axis=1)

data1.columns

"""##Creating user defined function to carryout EDA."""

def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):
    """
    Boxplot and histogram combined
    """

    # Force numeric safety
    series = pd.to_numeric(data[feature], errors="coerce")

    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,
        sharex=True,
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )

    sns.boxplot(
        x=series,
        ax=ax_box2,
        showmeans=True,
        color="violet"
    )

    # Histogram (bins only passed if provided)
    if bins is not None:
        sns.histplot(series, kde=kde, ax=ax_hist2, bins=bins)
    else:
        sns.histplot(series, kde=kde, ax=ax_hist2)

    ax_hist2.axvline(series.mean(), color="green", linestyle="--")
    ax_hist2.axvline(series.median(), color="black", linestyle="-")

# function to create labeled barplots


def labeled_barplot(data, feature, perc=True, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data1[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 2, 6))
    else:
        plt.figure(figsize=(n + 2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data1,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

#Defining function to plot stacked barplot
def stacked_barplot(data, x, y, normalize=False):
    """
    Stacked bar plot for two categorical variables
    """

    # Defensive check
    if x not in data.columns or y not in data.columns:
        raise KeyError(f"{x} or {y} not found in dataframe")

    ct = pd.crosstab(data[x], data[y], normalize="index" if normalize else False)

    ct.plot(
        kind="bar",
        stacked=True,
        figsize=(8, 5)
    )

    plt.xlabel(x)
    plt.ylabel("Proportion" if normalize else "Count")
    plt.title(f"{x} vs {y}")
    plt.legend(title=y)

# function to plot distributions wrt target


def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    target_uniq = data1[target].unique()

    axs[0, 0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data1[data1[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0, 0],
        color="teal",
        stat="density",
    )

    axs[0, 1].set_title("Distribution of target for target=" + str(target_uniq[1]))
    sns.histplot(
        data=data1[data1[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0, 1],
        color="orange",
        stat="density",
    )

    axs[1, 0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data1, x=target, y=predictor, ax=axs[1, 0], palette="gist_rainbow")

    axs[1, 1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1, 1],
        showfliers=False,
        palette="gist_rainbow",
    )

    plt.tight_layout()
    plt.show()

"""##Univariate Analysis"""

# Check for continuous data types
continuous_cols = data.select_dtypes(include=[np.number])  # Select numeric columns

# Loop through continuous columns
for col in continuous_cols.columns:
    plt.figure()  # Create a new figure for each plot
    histogram_boxplot(data1, col)  # Pass the column name (feature)

    # Extract the column title from the DataFrame
    col_title = data1.columns.get_loc(col)  # Get the column index and use it to retrieve the title
    plt.title(col_title)  # Set the plot title using the column title
    plt.show()

"""- The largest number of customers(1200) pay the least montly charge(20).
- The largest number of customers(about 1200) spent less than a month with the company.

###Visalizing features with categorical variables
"""

# Check for categorical data types
categorical_cols = data1.select_dtypes(include=['object'])  # Select category columns (categorical data)


# Loop through categorical columns
for col in categorical_cols.columns:
   plt.figure()  # Create a new figure for each plot

   labeled_barplot(data, col)  # Plot bar chart of categorical variables

   plt.show();

"""##Bivariate analysis"""

data2 = data1.copy()

# Converting Boolean variables to binary coding
data2['Churn'] = data2['Churn'].replace({'Yes': 1, 'No': 0})
data2['Partner'] = data2['Partner'].replace({'Yes': 1, 'No': 0})
data2['Dependents'] = data2['Dependents'].replace({'Yes': 1, 'No': 0})
data2['PhoneService'] = data2['PhoneService'].replace({'Yes': 1, 'No': 0})
data2['PaperlessBilling'] = data2['PaperlessBilling'].replace({'Yes': 1, 'No': 0})

# One-hot encoding non-ordinal features
def create_dummies(data2, specific_cols):
    """
    Creates dummy variables for specific columns in a pandas DataFrame.

    Args:
        data: The pandas DataFrame.
        specific_cols (list): A list of column names to convert to dummy variables.

    Returns:
        pandas.DataFrame: The DataFrame with new dummy variables.
    """

    # Create dummy variables for the specified columns
    dummies = pd.get_dummies(data2[specific_cols], drop_first=False)

    # Drop the original columns from the DataFrame
    data2 = data2.drop(columns=specific_cols)

    # Combine original data without the specified columns with dummy variables
    return pd.concat([data2, dummies], axis=1)


specific_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity','OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']  # Columns to convert to dummies
data2 = create_dummies(data2, specific_cols)  # Saving dummy variables

data2 = data2.replace({"Yes": 1, "No": 0})

data2.head()

"""###Corelation Heatmap"""

# creating a list of numerical columns
num_cols = data2.select_dtypes(include=[np.number, bool]).columns.tolist()

#plotting correlation heat map
plt.figure(figsize=(30, 10))
sns.heatmap(
    data2[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")

plt.show()

"""###Bivariate analysis"""

plt.figure()
sns.boxplot(data = data1,x = 'Churn' , y = 'tenure')
plt.title('Boxplot of Churn by tenure(months)')
plt.tight_layout()
plt.show();

"""- 10-18 months appears to be a very critical period. Customers who spend at least 10-18 months with the company are least likely to Churn."""

plt.figure()
sns.boxplot(data = data1,x = 'Churn' , y = 'MonthlyCharges')
plt.title('Boxplot of Churn by MonthlyCharges')
plt.tight_layout()
plt.show();

"""- Customers who pay at least 80 dollars monthly are most likely to churn."""

sns.lineplot(data=data1, x='tenure', y='MonthlyCharges')

plt.title('Trend of Monthly Charges over Tenure')
plt.tight_layout()

# Save the output
plt.savefig('tenure_monthly_charges_line.png')

"""- It appears the longer the customers remain with company, the more likely they to subscribe to more services and incur more montly charge.
- Infact it appear loyal customers are willing to spend more.
"""

plt.figure()
plt.show();
stacked_barplot(data,'Churn', 'InternetService')
plt.show();

"""- It shows customers with Fiber optic internet service are likelier to churn."""

plt.figure()
plt.show();
stacked_barplot(data,'Churn', 'DeviceProtection')
plt.show();

"""- Customers who do not subscribe to device protection service are more likely to churn."""

plt.figure()
stacked_barplot(data,'Contract', 'Churn')
plt.show();

"""- Customers who subscribe to month-to-month contracts are most likely to churn."""

plt.figure()
stacked_barplot(data,'Contract', 'Partner')
plt.show();

"""- Customers with no partners are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'Dependents')

plt.show();

"""Customers with no dependents are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'OnlineSecurity')

plt.show();

"""- Customers who do not subcribe to online security service are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'OnlineBackup')
plt.show();

"""- Customers who do not subcribe to online backup service are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'TechSupport')
plt.show();

"""- Customers who do not use tech support service are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'StreamingMovies')
plt.show();

"""- Customers who do not subcribe to streaming movies service are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'StreamingTV')
plt.show();

"""- Customers who do not subcribe to streaming tv services are most likely to subscribe to the month-to-month contract plans."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'PaymentMethod')
plt.show();

"""- Customers whose payment method is electronic check are most likely to subscribe to the month-to-month contract plans.
- While those subscribed to two years contracts are most likely to use a bank transfer method.

"""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'PhoneService')
plt.show();

"""- No notable findings."""

plt.figure()
plt.show();
stacked_barplot(data,'Contract', 'InternetService')
plt.show();

"""- Customers with month to month contracts are most likely to use fiber optics internet service.

# Performing data cleaning and preparing for Modeling

###Data cleaning

- 11 blank data points where observed in 'TotalCharges' column of the dataset, this made the column to be saved as an object instead of a float.
- It was also observed that only customer with a tenure of 0(new customers) have blank TotalCharges, these are likely new customers and have not made any payments so far.
- Hence I will be applying 0 as TotalCharge to this group of customers.
- Unique Identifier column will also be dropped as its useless in modeling data, 'CustomerID'
- There are no other issues in the dataset that requires cleaning.
"""

#dropping unique identifier column 'customerID'
data = data.drop('customerID', axis=1)

#Imputing zero to the 11 new customer with blank TotalCharge value
data.loc[data["tenure"] == 0, "TotalCharges"] = 0

"""Checking again for missing or blank values in TotalCharge column."""

missing_total_charges = (
    data["TotalCharges"].isnull().sum()
    + (data["TotalCharges"].str.strip() == "").sum()
)

print(f"There are {missing_total_charges} missing or blank values in the TotalCharges column")

"""Checking data info again"""

data.info()

"""- Even after handling the issues in the TotalCharges column, the column is still been stored as an Object instead of a float.
- The TotalCharge column will be manually tranformed to a float.
"""

#Transforming TotalCharges from Object to float data type
data["TotalCharges"] = data["TotalCharges"].astype(float)

data.info()

"""- Total charges column is now correctly stored as a float.

Checking statistical summary again.
"""

#Checking statistical summary
data.describe(include='all').T

"""- The statistical summary of total charge is now represented properly.

###Feature Engineering

As Observed during preliminary EDA, there seems to be a group of customers who have no dependents or partners. Thus are most likely students, young adults, or early professionals with no family commitments.
This segment of customers prefer month-to-month contracts,are techy sarvy(do not require tech support services), prefer fast internet services like fiber optic internet, and hardly stays with the company beyong a year and spend a lot monthly because they choose month-to-month contract which leads to them missing a lot of offers juicy associated with longer contract plans.

I will be creating a feature called "LowCommitment_SingleHousehold" to capture this unique demographic.
"""

data["LowCommitment_SingleHousehold"] = (
    (data["Contract"] == "Month-to-month") &
    (data["Dependents"] == "No") &
    (data["Partner"] == "No")
).astype(int)

"""###Preparing data for modeling"""

#Creating copy of cleaned dataset for medeling preparation
df_final = data.copy()

# Converting Boolean variables to binary coding
df_final['Churn'] = df_final['Churn'].replace({'Yes': 1, 'No': 0})
df_final['Partner'] = df_final['Partner'].replace({'Yes': 1, 'No': 0})
df_final['Dependents'] = df_final['Dependents'].replace({'Yes': 1, 'No': 0})
df_final['PhoneService'] = df_final['PhoneService'].replace({'Yes': 1, 'No': 0})
df_final['PaperlessBilling'] = df_final['PaperlessBilling'].replace({'Yes': 1, 'No': 0})

# One-hot encoding non-ordinal features
def create_dummies(df_final, specific_cols):
    """
    Creates dummy variables for specific columns in a pandas DataFrame.

    Args:
        data: The pandas DataFrame.
        specific_cols (list): A list of column names to convert to dummy variables.

    Returns:
        pandas.DataFrame: The DataFrame with new dummy variables.
    """

    # Create dummy variables for the specified columns
    dummies = pd.get_dummies(df_final[specific_cols], drop_first=False)

    # Drop the original columns from the DataFrame
    df_final = df_final.drop(columns=specific_cols)

    # Combine original data without the specified columns with dummy variables
    return pd.concat([df_final, dummies], axis=1)


specific_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity','OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']  # Columns to convert to dummies
df_final = create_dummies(df_final, specific_cols)  # Saving dummy variables

df_final = df_final.replace({"Yes": 1, "No": 0})

df_final.head()

X = df_final.drop(['Churn'],axis=1)
y = df_final['Churn']

X = pd.get_dummies(X,drop_first=True)

# Splitting data into training and test set:
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)

y.value_counts(1)

"""- 26.5 % OF CUSTOMERS CHURNED"""

y_test.value_counts(1)



"""#Final Exploratory Data Analysis

##Univariate analysis
"""

# Check for continuous data types
continuous_cols = df_final.select_dtypes(include=[np.number])  # Select numeric columns

# Loop through continuous columns
for col in continuous_cols.columns:
  if df_final[col].dtype == object:
        continue
  plt.figure()  # Create a new figure for each plot
  histogram_boxplot(df_final, col)  # Pass the column name (feature)

    # Extract the column title from the DataFrame
  col_title = df_final.columns.get_loc(col)  # Get the column index and use it to retrieve the title
  plt.title(col_title)  # Set the plot title using the column title
  plt.show()

"""- The median totalCharges paid is 2200 dollars.
- TotalChrges is heavily skewed to the left, with the largest proprtion of customers paying close to 0 totalCharge(these might be new customers).
"""



"""##Bivariate Analysis

###Plotting Correlation heatmap
"""

# creating a list of numerical columns
num_cols = df_final.select_dtypes(include=[np.number, bool]).columns.tolist()

#plotting correlation heat map
plt.figure(figsize=(30, 10))
sns.heatmap(
    df_final[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")

plt.show()

"""###TotalCharge vs Churn"""

plt.figure()
sns.boxplot(data = df_final,x = 'Churn' , y = 'TotalCharges')
plt.title('Boxplot of Churn by TotalCharges')
plt.tight_layout()
plt.show();

"""- It appears customers who pay less than 2000 dollars are more likely to churn.
- This might indicate that these customers do not stay long enough in the company to accrue higher TotalCharge.

###TotalCharge vs Partner
"""

plt.figure()
sns.boxplot(data = df_final,x = 'Partner' , y = 'TotalCharges')
plt.title('Boxplot of TotalCharges by Partner')
plt.tight_layout()
plt.show();

"""- Customers with partners are more likely to spend over 2000dollars at the end of their contract.
- Again, this point to committed customers with families, they are more likely to pay for more services and spend more time with the company by subscribing to longer services thus accruing more TotalCharge during the period.

###TotalCharges vs MultipleLines_yes
"""

plt.figure()
sns.boxplot(data = df_final,x = 'MultipleLines_Yes' , y = 'TotalCharges')
plt.title('Boxplot of TotalCharges by MultipleLines_Yes')
plt.tight_layout()
plt.show();

"""- Customers who spend more than 2000 dollars in Total Charge are very likely to thave multiple lines.
- Again, this points to a segment of commited customers with families(likely mid to senior profesionals, family men and women) that purchase extra lines for home and work, as well as , dependents.

###TotalCharge vs Multiplelines_No
"""

plt.figure()
sns.boxplot(data = df_final,x = 'MultipleLines_No' , y = 'TotalCharges')
plt.title('Boxplot of TotalCharges by MultipleLines_No')
plt.tight_layout()
plt.show();

"""- Reverse of the findings of the last charge.
- Likely reflecting the non commited_non dependent customer segment.

###Churn vs MultipleLines_Yes
"""

plt.figure()
plt.show();
stacked_barplot( df_final,'MultipleLines_Yes', 'Churn')
plt.show();

"""-There is no direct link between churn and multiple lines.

###Churn vs MultipleLines_No
"""

plt.figure()
plt.show();
stacked_barplot( df_final,'MultipleLines_No', 'Churn')
plt.show();

"""-There is no direct link between churn and multiple lines.

###Churn vs LowCommitment_SingleHousehold
"""



"""- There is 50% chance of LowCommitment_SingleHousehold to churn compared to the opposite groups whose chances are just  20%.

###Boxplot of LowCommitment_SingleHousehold by MonthlyCharges
"""

plt.figure()
sns.boxplot(data = df_final,x = 'LowCommitment_SingleHousehold' , y = 'MonthlyCharges')
plt.title('Boxplot of LowCommitment_SingleHousehold by MonthlyCharges')
plt.tight_layout()
plt.show();

"""- The range of monthly charge for the non-LowCommitment_SingleHousehold segment is wider compared to LowCommitment_SingleHousehold(they spend as low as $20 dollars to as high as 80 dollars monthly). This might indicate that they have access to more discounts because of their loyalty and they also tend to spend more because they use more services monthly when compared LowCommitment_SingleHousehold customer segment.

###Boxplot of LowCommitment_SingleHousehold by TotalCharges
"""

plt.figure()
sns.boxplot(data = df_final,x = 'LowCommitment_SingleHousehold' , y = 'TotalCharges')
plt.title('Boxplot of LowCommitment_SingleHousehold by TotalCharges')
plt.tight_layout()
plt.show();

"""- The LowCommitment_SingleHousehold customer segment hardly spend up to $2000 at the end of their contract. This might be driven by the fact that they do not use a lot of additional services, they prefer month-to-month  contracts where they can easily skip months, and they hardly stay with company up to a year as revealed in my earlier visualizatons."""

plt.figure()
plt.show();
stacked_barplot( df_final,'MultipleLines_No', 'LowCommitment_SingleHousehold')
plt.show();

"""- No significant finding."""

plt.figure()
sns.boxplot(data = df_final,x = 'LowCommitment_SingleHousehold' , y = 'tenure')
plt.title('Boxplot of LowCommitment_SingleHousehold by tenure')
plt.tight_layout()
plt.show();

"""- The LowCommitment_SingleHousehold hard remains with the company beyond 20 months."""

plt.figure()
plt.show();
stacked_barplot( df_final,'LowCommitment_SingleHousehold','Contract_Month-to-month', )
plt.show();

"""- Clearly shows the preference of LowCommitment_SingleHousehold for month-to-month contracts."""

plt.figure()
plt.show();
stacked_barplot( df_final,'LowCommitment_SingleHousehold','Contract_One year' )
plt.show();

"""- One year contract is not popuplar among the  LowCommitment_SingleHousehold customer segment."""

plt.figure()
plt.show();
stacked_barplot( df_final,'LowCommitment_SingleHousehold','TechSupport_No' )
plt.show();

"""- The LowCommitment_SingleHousehold are tech sarvy and a hardly use additional services that might require tech supports. Hence their very little use of tech support services from this visualization."""

plt.figure(figsize=(8,5))
sns.boxplot(
    data=df_final,
    x="Contract_Month-to-month",
    y="MonthlyCharges",
    hue="LowCommitment_SingleHousehold"
)
plt.title("Monthly Charges by monthlyContract and Household Type")
plt.tight_layout()
plt.show()

"""- Clearly shows that if a customer under LowCommitment_SingleHousehold segment spends above $80 monthly, he is more likely to churn due to better offers from other company."""

churn_rate = (
    df_final
    .groupby("LowCommitment_SingleHousehold")["Churn"]
    .mean()
)

churn_rate.plot(kind="bar", title="Churn Rate by Household Commitment")
plt.ylabel("Churn Rate")
plt.show()

"""= Clearly show LowCommitment_SingleHousehold customer segment twice more likely to churn compared to the non-LowCommitment_SingleHousehold customer segment."""

plt.figure(figsize=(7,5))
sns.kdeplot(
    data=df_final,
    x="MonthlyCharges",
    hue="LowCommitment_SingleHousehold",
    fill=True,
    common_norm=False
)
plt.title("Distribution of Monthly Charges")
plt.show()

"""- Highlights a high density of LowCommitment_SingleHousehold who pay as high as $80 monthly, and a high density of non-LowCommitment_SingleHousehold who pay as low as $20 monthly due to the reasons highlighted earlier."""

plt.figure(figsize=(7,5))
sns.scatterplot(
    data=df_final,
    x="tenure",
    y="MonthlyCharges",
    hue="LowCommitment_SingleHousehold",
    alpha=0.5
)
plt.title("Tenure vs Monthly Charges")
plt.show()

"""-  The LowCommitment_SingleHousehold spend shorter time with the company and spend more."""

subset = df_final[df_final["LowCommitment_SingleHousehold"] == 1]

plt.figure(figsize=(7,5))
sns.boxplot(
    data=subset,
    x="Churn",
    y="MonthlyCharges"
)
plt.title("Monthly Charges vs Churn\n(Low-Commitment Single-Household Customers)")
plt.xlabel("Churn (0 = No, 1 = Yes)")
plt.tight_layout()
plt.show()

"""- Because the population that churn more often belong to the LowCommitment_SingleHousehold, those that most likely to churn usually spend at least $80 dollars monthly.
- As I mntioned earlier, becuase the LowCommitment_SingleHousehold miss out of discounts because of their preference for the more expensive month-to-month contracts they are more likely to spend more monthly.

#Model Building

##Model Evaluation Criteria

The nature of predictions made by the models will translate as follows:

- True positives (TP) are customer churn correctly predicted by the model.
- False negatives (FN) are customers who actually churned but was not detected by the model.
- False positives (FP) are customer who remained with company in reality but was wrongly flagged as churn by the model.
- - True Negative (TN) are customer who did not and was correctly predicted by the model

**Which metric to optimize?**

- We need to choose the metric which will ensure that caters well for Recall, thus reducing false negative to bare minimum.
- A model with strong Recall performance will ensure that we don't miss out any customer will likely be lost, and campaigns and promos to keep them will be prioritized.
- There is also consideration for the possibility of the model incorrectly flagging churn for a client who will not churn. This might lead to wasting money to keep customers who would not have even churned. This can carry particularly financial consequences if the campaigns are expensive.
- This indicates my model should have a good balance between precision and Recall, just to address my concerns.
- Another thing to consider, is that the prediction class is markedly imbalanced, hence whatever metrics to be choosen should work well with Imbalance prediction class. The "Precision-Recall Area under the curve"(PR-AUC) metric appears to  be the best metric for predicting imbalanced prediction class in churn analysis.
- For this project, my primary metric will be ''PR-AUC". However, I will also test my model performance with Recall, precision, accuracy, ROC-AUC, and F1-SCORE for COMPARISM and tuning purposes.

**Let's define a function to output different metrics  on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1

        },
        index=[0],
    )

    return df_perf

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf

"""##Model Building with original data

I will be building multiple models on training data, I will use the K-FOLD cross validation to assess performance on the training set, while validating performance on a validation set.

Building multiple models with original data

I will be building the following models

  1. Lasso
  2. Ridge
  3. Adaboost
  4. Bagging
  5. GBM
  6. XG boost
  7. RandomForest
  8. Logistic Regression
  9. Decision Tree
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import StratifiedKFold, cross_validate
# from sklearn.metrics import (
#     accuracy_score,
#     precision_score,
#     recall_score,
#     f1_score,
#     roc_auc_score,
#     average_precision_score
# )
# 
# models = []
# 
# models.append(("dtree", DecisionTreeClassifier(random_state=1)))
# models.append(("Bagging", BaggingClassifier(random_state=1)))
# models.append(("Random forest", RandomForestClassifier(random_state=1)))
# models.append(("GBM", GradientBoostingClassifier(random_state=1)))
# models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
# models.append(("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")))
# 
# models.append(("RIDGE", RidgeClassifier(random_state=1, alpha=0.3)))
# models.append(("LOGISTIC REGRESSION", LogisticRegression(random_state=1)))
# models.append(("Lasso", LogisticRegression(random_state=1, penalty="l1", solver="liblinear", C=3.0)))
# 
# # ================================
# # CROSS-VALIDATION METRICS
# # ================================
# scoring = {
#     "Accuracy": "accuracy",
#     "Precision": "precision",
#     "Recall": "recall",
#     "F1": "f1",
#     "ROC_AUC": "roc_auc",
#     "PR_AUC": "average_precision",
# }
# 
# print("\nCross-Validation performance on training dataset:\n")
# 
# for name, model in models:
#     kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
# 
#     cv_results = cross_validate(
#         estimator=model,
#         X=X_train,
#         y=y_train,
#         scoring=scoring,
#         cv=kfold
#     )
# 
#     print(
#         f"{name} | "
#         f"Acc: {cv_results['test_Accuracy'].mean():.3f} | "
#         f"Prec: {cv_results['test_Precision'].mean():.3f} | "
#         f"Recall: {cv_results['test_Recall'].mean():.3f} | "
#         f"F1: {cv_results['test_F1'].mean():.3f} | "
#         f"ROC-AUC: {cv_results['test_ROC_AUC'].mean():.3f} | "
#         f"PR-AUC: {cv_results['test_PR_AUC'].mean():.3f}"
#     )
# 
# # ================================
# # TEST SET METRICS
# # ================================
# print("\nValidation Performance (Test Set):\n")
# 
# for name, model in models:
#     model.fit(X_train, y_train)
# 
#     y_pred = model.predict(X_test)
# 
#     # Probabilities / decision scores
#     if hasattr(model, "predict_proba"):
#         y_scores = model.predict_proba(X_test)[:, 1]
#     else:
#         y_scores = model.decision_function(X_test)
# 
#     acc = accuracy_score(y_test, y_pred)
#     prec = precision_score(y_test, y_pred)
#     recall = recall_score(y_test, y_pred)
#     f1 = f1_score(y_test, y_pred)
#     roc_auc = roc_auc_score(y_test, y_scores)
#     pr_auc = average_precision_score(y_test, y_scores)
# 
#     print(
#         f"{name} | "
#         f"Acc: {acc:.3f} | "
#         f"Prec: {prec:.3f} | "
#         f"Recall: {recall:.3f} | "
#         f"F1: {f1:.3f} | "
#         f"ROC-AUC: {roc_auc:.3f} | "
#         f"PR-AUC: {pr_auc:.3f}"
#     )
#

results_nosampling = []

for name, model in models:   #

    # Fit model on original training data
    model.fit(X_train, y_train)

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Probabilities / decision scores
    if hasattr(model, "predict_proba"):
        y_train_prob = model.predict_proba(X_train)[:, 1]
        y_test_prob = model.predict_proba(X_test)[:, 1]
    else:
        y_train_prob = model.decision_function(X_train)
        y_test_prob = model.decision_function(X_test)

    results_nosampling.append({
        "Model": name,

        # ---- TRAIN METRICS ----
        "Train Recall": recall_score(y_train, y_train_pred),
        "Train Precision": precision_score(y_train, y_train_pred),
        "Train F1": f1_score(y_train, y_train_pred),
        "Train Accuracy": accuracy_score(y_train, y_train_pred),
        "Train ROC-AUC": roc_auc_score(y_train, y_train_prob),
        "Train PR-AUC": average_precision_score(y_train, y_train_prob),

        # ---- TEST METRICS ----
        "Test Recall": recall_score(y_test, y_test_pred),
        "Test Precision": precision_score(y_test, y_test_pred),
        "Test F1": f1_score(y_test, y_test_pred),
        "Test Accuracy": accuracy_score(y_test, y_test_pred),
        "Test ROC-AUC": roc_auc_score(y_test, y_test_prob),
        "Test PR-AUC": average_precision_score(y_test, y_test_prob),
    })

# Create DataFrame
performance_table_nosampling = pd.DataFrame(results_nosampling)

# Sort by Test PR-AUC (best churn metric)
performance_table_nosampling = performance_table_nosampling.sort_values(
    by="Test PR-AUC", ascending=False
).reset_index(drop=True)

performance_table_nosampling

"""- Adaboost model is the best model, with the best performance and least overfitting here in the PR-AUC metrics.

##Model Building with Oversampled data

- Since the predictor class is imbalanced, I want to experiment the models on oversampled data set using SMOTE.
"""

models_oversampled = [
    ("dtree", DecisionTreeClassifier(random_state=1)),
    ("Bagging", BaggingClassifier(random_state=1)),
    ("Random forest", RandomForestClassifier(random_state=1)),
    ("GBM", GradientBoostingClassifier(random_state=1)),
    ("Adaboost", AdaBoostClassifier(random_state=1)),
    ("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")),
    ("RIDGE", RidgeClassifier(random_state=1, alpha=0.3)),
    ("LASSO", LogisticRegression(random_state=1, penalty="l1", solver="liblinear", C=3.0)),
    ("LOGISTIC REGRESSION", LogisticRegression(random_state=1)),
]

scoring = {
    "recall": "recall",
    "precision": "precision",
    "f1": "f1",
    "accuracy": "accuracy",
    "roc_auc": "roc_auc",
    "pr_auc": "average_precision"
}

print("\nCross-Validation Performance (SMOTE applied correctly):\n")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

for name, model in models_oversampled:
    pipeline = Pipeline(
        steps=[
            ("smote", SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)),
            ("model", model),
        ]
    )

    cv_results = cross_validate(
        estimator=pipeline,
        X=X_train,
        y=y_train,
        cv=cv,
        scoring=scoring,
        n_jobs=-1
    )

    print(f"{name}")
    print(f"  Recall     : {cv_results['test_recall'].mean():.4f}")
    print(f"  Precision  : {cv_results['test_precision'].mean():.4f}")
    print(f"  F1-score   : {cv_results['test_f1'].mean():.4f}")
    print(f"  Accuracy   : {cv_results['test_accuracy'].mean():.4f}")
    print(f"  ROC-AUC    : {cv_results['test_roc_auc'].mean():.4f}")
    print(f"  PR-AUC     : {cv_results['test_pr_auc'].mean():.4f}\n")

print("\nValidation Performance (SMOTE-trained models):\n")

for name, model in models_oversampled:
    pipeline = Pipeline(
        steps=[
            ("smote", SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)),
            ("model", model),
        ]
    )

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)
    y_prob = pipeline.predict_proba(X_test)[:, 1] if hasattr(
        pipeline.named_steps["model"], "predict_proba"
    ) else None

    print(f"{name}")
    print(f"  Recall     : {recall_score(y_test, y_pred):.4f}")
    print(f"  Precision  : {precision_score(y_test, y_pred):.4f}")
    print(f"  F1-score   : {f1_score(y_test, y_test):.4f}")
    print(f"  Accuracy   : {accuracy_score(y_test, y_pred):.4f}")

    if y_prob is not None:
        print(f"  ROC-AUC    : {roc_auc_score(y_test, y_prob):.4f}")
        print(f"  PR-AUC     : {average_precision_score(y_test, y_prob):.4f}")

    print()

results = []

for name, model in models_oversampled:

    # SMOTE only applied to TRAINING data
    pipeline = Pipeline(
        steps=[
            ("smote", SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)),
            ("model", model),
        ]
    )

    # Fit on training data
    pipeline.fit(X_train, y_train)

    # Predictions
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    # Probabilities (needed for AUCs)
    if hasattr(pipeline.named_steps["model"], "predict_proba"):
        y_train_prob = pipeline.predict_proba(X_train)[:, 1]
        y_test_prob = pipeline.predict_proba(X_test)[:, 1]
    else:
        y_train_prob = pipeline.decision_function(X_train)
        y_test_prob = pipeline.decision_function(X_test)

    results.append({
        "Model": name,

        # ---- TRAIN METRICS ----
        "Train Recall": recall_score(y_train, y_train_pred),
        "Train Precision": precision_score(y_train, y_train_pred),
        "Train F1": f1_score(y_train, y_train_pred),
        "Train Accuracy": accuracy_score(y_train, y_train_pred),
        "Train ROC-AUC": roc_auc_score(y_train, y_train_prob),
        "Train PR-AUC": average_precision_score(y_train, y_train_prob),

        # ---- TEST METRICS ----
        "Test Recall": recall_score(y_test, y_test_pred),
        "Test Precision": precision_score(y_test, y_test_pred),
        "Test F1": f1_score(y_test, y_test_pred),
        "Test Accuracy": accuracy_score(y_test, y_test_pred),
        "Test ROC-AUC": roc_auc_score(y_test, y_test_prob),
        "Test PR-AUC": average_precision_score(y_test, y_test_prob),
    })

# Create comparison table
performance_table = pd.DataFrame(results)

# Sort by Test PR-AUC (best metric for churn)
performance_table = performance_table.sort_values(
    by="Test PR-AUC", ascending=False
).reset_index(drop=True)

performance_table

"""- With the exception of Decision tree, bagging, and RandomForest every other model seems to be performing well with little overfitting.
- Lasso model seems to be the best performing here with the best performance on PR-AUC metric and very little overfitting when compared to the GBM model.

##Model Building with Undersampled Data
"""

models_un = [
    ("dtree", DecisionTreeClassifier(random_state=1)),
    ("Bagging", BaggingClassifier(random_state=1)),
    ("Random forest", RandomForestClassifier(random_state=1)),
    ("GBM", GradientBoostingClassifier(random_state=1)),
    ("Adaboost", AdaBoostClassifier(random_state=1)),
    ("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")),
    ("RIDGE", RidgeClassifier(random_state=1, alpha=0.3)),
    ("LASSO", LogisticRegression(random_state=1, penalty="l1", solver="liblinear", C=3.0)),
    ("LOGISTIC REGRESSION", LogisticRegression(random_state=1)),
    ]

print("\nCross-Validation Performance (Random Undersampling applied correctly):\n")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

for name, model in models_un:

    pipeline = Pipeline(
        steps=[
            ("undersample", RandomUnderSampler(
                sampling_strategy=1, random_state=1
            )),
            ("model", model),
        ]
    )

    cv_results = cross_validate(
        estimator=pipeline,
        X=X_train,
        y=y_train,
        cv=cv,
        scoring=scoring,
        n_jobs=-1
    )

    print(f"{name}")
    print(f"  Recall     : {cv_results['test_recall'].mean():.4f}")
    print(f"  Precision  : {cv_results['test_precision'].mean():.4f}")
    print(f"  F1-score   : {cv_results['test_f1'].mean():.4f}")
    print(f"  Accuracy   : {cv_results['test_accuracy'].mean():.4f}")
    print(f"  ROC-AUC    : {cv_results['test_roc_auc'].mean():.4f}")
    print(f"  PR-AUC     : {cv_results['test_pr_auc'].mean():.4f}\n")

results_undersampling = []

for name, model in models_un:

    pipeline = Pipeline(
        steps=[
            ("undersample", RandomUnderSampler(
                sampling_strategy=1, random_state=1
            )),
            ("model", model),
        ]
    )

    # Fit on training data
    pipeline.fit(X_train, y_train)

    # Predictions
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    # Probabilities / decision scores
    if hasattr(pipeline.named_steps["model"], "predict_proba"):
        y_train_prob = pipeline.predict_proba(X_train)[:, 1]
        y_test_prob = pipeline.predict_proba(X_test)[:, 1]
    else:
        y_train_prob = pipeline.decision_function(X_train)
        y_test_prob = pipeline.decision_function(X_test)

    results_undersampling.append({
        "Model": name,

        # ---- TRAIN METRICS ----
        "Train Recall": recall_score(y_train, y_train_pred),
        "Train Precision": precision_score(y_train, y_train_pred),
        "Train F1": f1_score(y_train, y_train_pred),
        "Train Accuracy": accuracy_score(y_train, y_train_pred),
        "Train ROC-AUC": roc_auc_score(y_train, y_train_prob),
        "Train PR-AUC": average_precision_score(y_train, y_train_prob),

        # ---- TEST METRICS ----
        "Test Recall": recall_score(y_test, y_test_pred),
        "Test Precision": precision_score(y_test, y_test_pred),
        "Test F1": f1_score(y_test, y_test_pred),
        "Test Accuracy": accuracy_score(y_test, y_test_pred),
        "Test ROC-AUC": roc_auc_score(y_test, y_test_prob),
        "Test PR-AUC": average_precision_score(y_test, y_test_prob),
    })

# Create comparison table
performance_table_undersampling = pd.DataFrame(results_undersampling)

# Sort by Test PR-AUC (primary churn metric)
performance_table_undersampling = performance_table_undersampling.sort_values(
    by="Test PR-AUC", ascending=False
).reset_index(drop=True)

performance_table_undersampling

"""- Lasso model stands out as the best model here.

#Final Model

- Across my model experiments with the different sampled and none smapled data, Adaboost model stood out as the best perfoming model of 66% with PR-AUC metrics, it also scales well with unseen data.
- As I mentioned earlier, I choose the PR-AUC metrics because of its high suitability for classification problems whereby the predictor class is highly imbalanced,as experienced in this data set.
"""

Adaboost = AdaBoostClassifier(random_state=1)

final_model = Adaboost.fit(X_test, y_test)

feature_names = X_test.columns
importances = final_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""- Tenure lenght of the customer, customer selecting month-to-month contract, and monthly charges incured has been highlighted as the strongest predictor to Churn by my best model.

#Business Insights and Recommendations

#**Executive Summary**

The 26.5% churn rate observed in this dataset is systematic rather than random. Churn is primarily driven by the interaction of contract flexibility, tenure, and price sensitivity, with the highest risk concentrated among single-household customers with low contractual commitment.

The best-performing model (AdaBoost) identified tenure, month-to-month contracts, and monthly charges as the strongest predictors of churn. This aligns with insights from exploratory analysis: single-household customersâ€”typically without partners or dependentsâ€”prefer month-to-month contracts, adopt fewer bundled services, and are often highly price-sensitive despite paying relatively higher monthly charges.

This segment churns at nearly twice the rate of family-oriented customers, with churn risk peaking when monthly charges approach or exceed $80, particularly within the 10â€“18 month tenure window. Their preference for short-term contracts likely reflects income uncertainty and a desire for flexibility, increasing their responsiveness to price changes.

#Core Business Insights
1. **Low commitment strongly increases churn risk**

Month-to-month contracts consistently exhibit higher churn

Short-tenure customers churn significantly more frequently

Contract flexibility amplifies price sensitivity

âž¡ï¸ Low switching costs accelerate churn


2. **Price pressure disproportionately affects single-household customers**

Single-household, low-commitment customers:

Pay higher average MonthlyCharges

Exhibit sharply higher churn at upper price bands

Show a clear price threshold effect around $80


âž¡ï¸ These are not low-value customers; they are price-sensitive customers paying premium prices without commitment incentives


3. **Churn peaks when high prices meet low switching barriers**

The highest churn risk occurs when customers have:

Month-to-month contracts

Short tenure

High MonthlyCharges

No partner or dependents

âž¡ï¸ Churn is driven by misaligned pricing for flexibility-seeking customers


#Strategic Business Recommendations

**1.Targeted Retention for Single-Household, Low-Commitment Customers (Highest Priority)**

Target segment:

Single-household customers

Month-to-month contracts

Tenure below 12â€“18 months

MonthlyCharges above $80

Actions:

- Offer early contract-conversion incentives (e.g., 6-month plans with modest discounts)

- Deploy price-relief offers when customers cross churn-sensitive price thresholds

- Trigger retention campaigns before renewal or billing cycles

- Retention efforts should focus on perceived value rather than long-term lock-in.

**2. Introduce â€œCommitment-Lightâ€ Pricing Options**

Many single-household customers value flexibility but still require price fairness.

Recommendations:

- Introduce 3-month and 6-month contract options

- Offer price-stabilized month-to-month plans

- Bundle flexibility with light discounts or service credits

ðŸŽ¯ This reduces churn while respecting customersâ€™ need for short-term commitment.


**3. Operationalize Price Sensitivity as an Early-Warning Signal**

MonthlyCharges should be treated as a churn risk indicator, not just a revenue metric.

Implementation:

- Flag customersâ€”especially single-householdâ€”when MonthlyCharges exceed churn thresholds

- Proactively intervene with targeted promotions and plan reviews

- Monitor churn risk during the 10â€“18 month tenure window

**4. Improve Data & Customer Mix Strategy**

- Collect additional demographic indicators (e.g., age range, education level, household size) to refine segmentation and modeling

- Design acquisition campaigns aimed at family-oriented customers, who exhibit lower churn and higher lifetime value
"""